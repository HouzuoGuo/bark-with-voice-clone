{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "\n",
    "from bark.api import generate_audio\n",
    "from bark.generation import SAMPLE_RATE, preload_models, codec_decode, generate_coarse, generate_fine, generate_text_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_path = \"semantic_output/pytorch_model.bin\" # set to None if you don't want to use finetuned semantic\n",
    "coarse_path = \"coarse_output/pytorch_model.bin\" # set to None if you don't want to use finetuned coarse\n",
    "fine_path = \"fine_output/pytorch_model.bin\" # set to None if you don't want to use finetuned fine\n",
    "use_rvc = False # Set to False to use bark without RVC\n",
    "rvc_name = 'mi-test'\n",
    "rvc_path = f\"Retrieval-based-Voice-Conversion-WebUI/weights/{rvc_name}.pth\"\n",
    "index_path = f\"Retrieval-based-Voice-Conversion-WebUI/logs/{rvc_name}/added_IVF256_Flat_nprobe_1_{rvc_name}_v2.index\"\n",
    "device=\"cuda:0\"\n",
    "is_half=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_models(\n",
    "    text_use_gpu=True,\n",
    "    text_use_small=False,\n",
    "    text_model_path=semantic_path,\n",
    "    coarse_use_gpu=True,\n",
    "    coarse_use_small=False,\n",
    "    coarse_model_path=coarse_path,\n",
    "    fine_use_gpu=True,\n",
    "    fine_use_small=False,\n",
    "    fine_model_path=fine_path,\n",
    "    codec_use_gpu=True,\n",
    "    force_reload=False,\n",
    "    path=\"models\"\n",
    ")\n",
    "\n",
    "if use_rvc:\n",
    "    from rvc_infer import get_vc, vc_single\n",
    "    get_vc(rvc_path, device, is_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple generation\n",
    "text_prompt = \"I am Joe Biden... and this is the finetuned semantic, coarse and fine model! A lot better than the original!\"\n",
    "filepath = \"output/audio.wav\" # change this to your desired output path\n",
    "audio_array = generate_audio(text_prompt, history_prompt=None, text_temp=0.7, waveform_temp=0.7)\n",
    "write_wav(filepath, SAMPLE_RATE, audio_array)\n",
    "\n",
    "if use_rvc:\n",
    "    index_rate = 0.75\n",
    "    f0up_key = -6\n",
    "    filter_radius = 3\n",
    "    rms_mix_rate = 0.25\n",
    "    protect = 0.33\n",
    "    resample_sr = SAMPLE_RATE\n",
    "    f0method = \"harvest\" #harvest or pm\n",
    "    try:\n",
    "        audio_array = vc_single(0,filepath,f0up_key,None,f0method,index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
    "    except:\n",
    "        audio_array = vc_single(0,filepath,f0up_key,None,'pm',index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
    "    write_wav(filepath, SAMPLE_RATE, audio_array)\n",
    "\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_settings(text_prompt, semantic_temp=0.7, semantic_top_k=50, semantic_top_p=0.95, coarse_temp=0.7, coarse_top_k=50, coarse_top_p=0.95, fine_temp=0.5, voice_name=None, use_semantic_history_prompt=True, use_coarse_history_prompt=True, use_fine_history_prompt=True, output_full=False):\n",
    "    # generation with more control\n",
    "    x_semantic = generate_text_semantic(\n",
    "        text_prompt,\n",
    "        history_prompt=voice_name if use_semantic_history_prompt else None,\n",
    "        temp=semantic_temp,\n",
    "        top_k=semantic_top_k,\n",
    "        top_p=semantic_top_p,\n",
    "    )\n",
    "\n",
    "    x_coarse_gen = generate_coarse(\n",
    "        x_semantic,\n",
    "        history_prompt=voice_name if use_coarse_history_prompt else None,\n",
    "        temp=coarse_temp,\n",
    "        top_k=coarse_top_k,\n",
    "        top_p=coarse_top_p,\n",
    "    )\n",
    "    x_fine_gen = generate_fine(\n",
    "        x_coarse_gen,\n",
    "        history_prompt=voice_name if use_fine_history_prompt else None,\n",
    "        temp=fine_temp,\n",
    "    )\n",
    "\n",
    "    if output_full:\n",
    "        full_generation = {\n",
    "            'semantic_prompt': x_semantic,\n",
    "            'coarse_prompt': x_coarse_gen,\n",
    "            'fine_prompt': x_fine_gen,\n",
    "        }\n",
    "        return full_generation, codec_decode(x_fine_gen)\n",
    "    return codec_decode(x_fine_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"I am Joe Biden... and this is the finetuned semantic, coarse and fine model! A lot better than the original!\"\n",
    "filepath = \"output/audio.wav\" # change this to your desired output path\n",
    "\n",
    "audio_array = generate_with_settings(\n",
    "    text_prompt,\n",
    "    semantic_temp=0.7,\n",
    "    semantic_top_k=50,\n",
    "    semantic_top_p=0.99,\n",
    "    coarse_temp=0.7,\n",
    "    coarse_top_k=50,\n",
    "    coarse_top_p=0.95,\n",
    "    fine_temp=0.5,\n",
    "    voice_name=\"howard-clone-from-60sec-sample\",\n",
    "    use_semantic_history_prompt=False,\n",
    "    use_coarse_history_prompt=True,\n",
    "    use_fine_history_prompt=True,\n",
    "    output_full=False\n",
    ")\n",
    "\n",
    "write_wav(filepath, SAMPLE_RATE, audio_array)\n",
    "\n",
    "if use_rvc:\n",
    "    index_rate = 0.75\n",
    "    f0up_key = -6\n",
    "    filter_radius = 3\n",
    "    rms_mix_rate = 0.25\n",
    "    protect = 0.33\n",
    "    resample_sr = SAMPLE_RATE\n",
    "    f0method = \"harvest\" #harvest or pm\n",
    "    try:\n",
    "        audio_array = vc_single(0,filepath,f0up_key,None,f0method,index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
    "    except:\n",
    "        audio_array = vc_single(0,filepath,f0up_key,None,'pm',index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
    "    write_wav(filepath, SAMPLE_RATE, audio_array)\n",
    "\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_and_recombine_text(text, desired_length=100, max_length=150):\n",
    "    # from https://github.com/neonbjb/tortoise-tts\n",
    "    \"\"\"Split text it into chunks of a desired length trying to keep sentences intact.\"\"\"\n",
    "    # normalize text, remove redundant whitespace and convert non-ascii quotes to ascii\n",
    "    text = re.sub(r\"\\n\\n+\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[“”]\", '\"', text)\n",
    "\n",
    "    rv = []\n",
    "    in_quote = False\n",
    "    current = \"\"\n",
    "    split_pos = []\n",
    "    pos = -1\n",
    "    end_pos = len(text) - 1\n",
    "\n",
    "    def seek(delta):\n",
    "        nonlocal pos, in_quote, current\n",
    "        is_neg = delta < 0\n",
    "        for _ in range(abs(delta)):\n",
    "            if is_neg:\n",
    "                pos -= 1\n",
    "                current = current[:-1]\n",
    "            else:\n",
    "                pos += 1\n",
    "                current += text[pos]\n",
    "            if text[pos] == '\"':\n",
    "                in_quote = not in_quote\n",
    "        return text[pos]\n",
    "\n",
    "    def peek(delta):\n",
    "        p = pos + delta\n",
    "        return text[p] if p < end_pos and p >= 0 else \"\"\n",
    "\n",
    "    def commit():\n",
    "        nonlocal rv, current, split_pos\n",
    "        rv.append(current)\n",
    "        current = \"\"\n",
    "        split_pos = []\n",
    "\n",
    "    while pos < end_pos:\n",
    "        c = seek(1)\n",
    "        # do we need to force a split?\n",
    "        if len(current) >= max_length:\n",
    "            if len(split_pos) > 0 and len(current) > (desired_length / 2):\n",
    "                # we have at least one sentence and we are over half the desired length, seek back to the last split\n",
    "                d = pos - split_pos[-1]\n",
    "                seek(-d)\n",
    "            else:\n",
    "                # no full sentences, seek back until we are not in the middle of a word and split there\n",
    "                while c not in \"!?.\\n \" and pos > 0 and len(current) > desired_length:\n",
    "                    c = seek(-1)\n",
    "            commit()\n",
    "        # check for sentence boundaries\n",
    "        elif not in_quote and (c in \"!?\\n\" or (c == \".\" and peek(1) in \"\\n \")):\n",
    "            # seek forward if we have consecutive boundary markers but still within the max length\n",
    "            while (\n",
    "                pos < len(text) - 1 and len(current) < max_length and peek(1) in \"!?.\"\n",
    "            ):\n",
    "                c = seek(1)\n",
    "            split_pos.append(pos)\n",
    "            if len(current) >= desired_length:\n",
    "                commit()\n",
    "        # treat end of quote as a boundary if its followed by a space or newline\n",
    "        elif in_quote and peek(1) == '\"' and peek(2) in \"\\n \":\n",
    "            seek(2)\n",
    "            split_pos.append(pos)\n",
    "    rv.append(current)\n",
    "\n",
    "    # clean up, remove lines with only whitespace or punctuation\n",
    "    rv = [s.strip() for s in rv]\n",
    "    rv = [s for s in rv if len(s) > 0 and not re.match(r\"^[\\s\\.,;:!?]*$\", s)]\n",
    "\n",
    "    return rv\n",
    "\n",
    "def generate_with_settings(text_prompt, semantic_temp=0.7, semantic_top_k=50, semantic_top_p=0.95, coarse_temp=0.7, coarse_top_k=50, coarse_top_p=0.95, fine_temp=0.5, voice_name=None, use_semantic_history_prompt=True, use_coarse_history_prompt=True, use_fine_history_prompt=True, output_full=False):\n",
    "    # generation with more control\n",
    "    x_semantic = generate_text_semantic(\n",
    "        text_prompt,\n",
    "        history_prompt=voice_name if use_semantic_history_prompt else None,\n",
    "        temp=semantic_temp,\n",
    "        top_k=semantic_top_k,\n",
    "        top_p=semantic_top_p,\n",
    "    )\n",
    "\n",
    "    x_coarse_gen = generate_coarse(\n",
    "        x_semantic,\n",
    "        history_prompt=voice_name if use_coarse_history_prompt else None,\n",
    "        temp=coarse_temp,\n",
    "        top_k=coarse_top_k,\n",
    "        top_p=coarse_top_p,\n",
    "    )\n",
    "    x_fine_gen = generate_fine(\n",
    "        x_coarse_gen,\n",
    "        history_prompt=voice_name if use_fine_history_prompt else None,\n",
    "        temp=fine_temp,\n",
    "    )\n",
    "\n",
    "    if output_full:\n",
    "        full_generation = {\n",
    "            'semantic_prompt': x_semantic,\n",
    "            'coarse_prompt': x_coarse_gen,\n",
    "            'fine_prompt': x_fine_gen,\n",
    "        }\n",
    "        return full_generation, codec_decode(x_fine_gen)\n",
    "    return codec_decode(x_fine_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"What do people gain from all their labors at which they toil under the sun?\n",
    "Generations come and generations go, but the earth remains forever.\n",
    "The sun rises and the sun sets, and hurries back to where it rises.\n",
    "The wind blows to the south and turns to the north; round and round it goes, ever returning on its course.\n",
    "All streams flow into the sea, yet the sea is never full.\n",
    "To the place the streams come from, there they return again.\n",
    "All things are wearisome, more than one can say.\n",
    "The eye never has enough of seeing, nor the ear its fill of hearing.\n",
    "What has been will be again, what has been done will be done again;\n",
    "there is nothing new under the sun.\n",
    "Is there anything of which one can say, “Look! This is something new”?\n",
    "It was here already, long ago; it was here before our time.\n",
    "No one remembers the former generations, and even those yet to come will not be remembered by those who follow them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the text into smaller pieces then combine the generated audio\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# generation settings\n",
    "voice_name = \"howard-clone-from-60sec-sample\"\n",
    "out_filepath = 'audio/audio.wav'\n",
    "\n",
    "semantic_temp = 0.7\n",
    "semantic_top_k = 100\n",
    "semantic_top_p = 0.99\n",
    "\n",
    "coarse_temp = 0.7\n",
    "coarse_top_k = 100\n",
    "coarse_top_p = 0.95\n",
    "\n",
    "fine_temp = 0.7\n",
    "\n",
    "use_semantic_history_prompt = True\n",
    "use_coarse_history_prompt = True\n",
    "use_fine_history_prompt = True\n",
    "\n",
    "use_last_generation_as_history = False\n",
    "\n",
    "if use_rvc:\n",
    "    index_rate = 0.75\n",
    "    f0up_key = -6\n",
    "    filter_radius = 3\n",
    "    rms_mix_rate = 0.25\n",
    "    protect = 0.33\n",
    "    resample_sr = SAMPLE_RATE\n",
    "    f0method = \"harvest\" #harvest or pm\n",
    "\n",
    "texts = split_and_recombine_text(text)\n",
    "\n",
    "all_parts = []\n",
    "for i, text in tqdm(enumerate(texts), total=len(texts)):\n",
    "    print('generating for text', i, text)\n",
    "    full_generation, audio_array = generate_with_settings(\n",
    "        text,\n",
    "        semantic_temp=semantic_temp,\n",
    "        semantic_top_k=semantic_top_k,\n",
    "        semantic_top_p=semantic_top_p,\n",
    "        coarse_temp=coarse_temp,\n",
    "        coarse_top_k=coarse_top_k,\n",
    "        coarse_top_p=coarse_top_p,\n",
    "        fine_temp=fine_temp,\n",
    "        voice_name=voice_name,\n",
    "        use_semantic_history_prompt=use_semantic_history_prompt,\n",
    "        use_coarse_history_prompt=use_coarse_history_prompt,\n",
    "        use_fine_history_prompt=use_fine_history_prompt,\n",
    "        output_full=True\n",
    "    )\n",
    "    if use_last_generation_as_history:\n",
    "        # save to npz\n",
    "        os.makedirs('_temp', exist_ok=True)\n",
    "        np.savez_compressed(\n",
    "            '_temp/history.npz',\n",
    "            semantic_prompt=full_generation['semantic_prompt'],\n",
    "            coarse_prompt=full_generation['coarse_prompt'],\n",
    "            fine_prompt=full_generation['fine_prompt'],\n",
    "        )\n",
    "        voice_name = '_temp/history.npz'\n",
    "    write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n",
    "\n",
    "    if use_rvc:\n",
    "        try:\n",
    "            audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,f0method,index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
    "        except:\n",
    "            audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,'pm',index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
    "        write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n",
    "    all_parts.append(audio_array)\n",
    "\n",
    "audio_array = np.concatenate(all_parts, axis=-1)\n",
    "\n",
    "# save audio\n",
    "write_wav(out_filepath, SAMPLE_RATE, audio_array)\n",
    "\n",
    "# play audio\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
